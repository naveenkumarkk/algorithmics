{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: graphviz 0.20.3\n",
      "Uninstalling graphviz-0.20.3:\n",
      "  Successfully uninstalled graphviz-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall graphviz --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Using cached graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Using cached graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Thompson NFA...\n",
      "Thompson NFA saved as 'thompson_nfa.png'.\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def thompson_construction(re_string):\n",
    "    \"\"\"\n",
    "    Generates a Thompson NFA for the given regular expression, implementing '+' as 'RE.RE*'.\n",
    "    \"\"\"\n",
    "    dot = Digraph(format='png')\n",
    "    dot.attr(rankdir='LR')  # Left-to-right visualization\n",
    "\n",
    "    # States counter\n",
    "    state_count = 0\n",
    "\n",
    "    def new_state():\n",
    "        nonlocal state_count\n",
    "        state = f'q{state_count}'\n",
    "        state_count += 1\n",
    "        return state\n",
    "\n",
    "    # Start with initial and final states\n",
    "    start_state = new_state()\n",
    "    final_state = new_state()\n",
    "\n",
    "    # Stack for automata\n",
    "    stack = []\n",
    "\n",
    "    for char in re_string:\n",
    "        if char.isalpha():  # Add a transition for letters\n",
    "            s1 = new_state()\n",
    "            s2 = new_state()\n",
    "            dot.node(s1, s1, shape='circle')\n",
    "            dot.node(s2, s2, shape='circle')\n",
    "            dot.edge(s1, s2, label=char)\n",
    "            stack.append((s1, s2))\n",
    "        elif char == '|':  # Union\n",
    "            a2, a1 = stack.pop(), stack.pop()\n",
    "            s1 = new_state()\n",
    "            s2 = new_state()\n",
    "            dot.node(s1, s1, shape='circle')\n",
    "            dot.node(s2, s2, shape='circle')\n",
    "            dot.edge(s1, a1[0], label='ε')\n",
    "            dot.edge(s1, a2[0], label='ε')\n",
    "            dot.edge(a1[1], s2, label='ε')\n",
    "            dot.edge(a2[1], s2, label='ε')\n",
    "            stack.append((s1, s2))\n",
    "        elif char == '*':  # Kleene Star\n",
    "            a = stack.pop()\n",
    "            s1 = new_state()\n",
    "            s2 = new_state()\n",
    "            dot.node(s1, s1, shape='circle')\n",
    "            dot.node(s2, s2, shape='circle')\n",
    "            dot.edge(s1, a[0], label='ε')\n",
    "            dot.edge(s1, s2, label='ε')\n",
    "            dot.edge(a[1], a[0], label='ε')\n",
    "            dot.edge(a[1], s2, label='ε')\n",
    "            stack.append((s1, s2))\n",
    "        elif char == '+':  # One or More Repetition\n",
    "            a = stack.pop()\n",
    "            s1 = new_state()\n",
    "            s2 = new_state()\n",
    "            dot.node(s1, s1, shape='circle')\n",
    "            dot.node(s2, s2, shape='circle')\n",
    "            \n",
    "            # Implementing RE+ as RE.RE*\n",
    "            dot.edge(s1, a[0], label='ε')  # First occurrence of RE\n",
    "            dot.edge(a[1], a[0], label='ε')  # Loop for RE*\n",
    "            dot.edge(a[1], s2, label='ε')  # Transition to the final state\n",
    "            stack.append((s1, s2))\n",
    "\n",
    "    # Connect the resulting NFA to the start and final states\n",
    "    nfa_start, nfa_end = stack.pop()\n",
    "    dot.edge(start_state, nfa_start, label='ε')\n",
    "    dot.edge(nfa_end, final_state, label='ε')\n",
    "    dot.node(start_state, start_state, shape='circle', color='red')  # Start state\n",
    "    dot.node(final_state, final_state, shape='doublecircle', color='green')  # Final state\n",
    "\n",
    "    return dot\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    re_string = \"(aba|ba|aaa)+\"\n",
    "\n",
    "    # Generate Thompson NFA\n",
    "    print(\"Generating Thompson NFA...\")\n",
    "    thompson_nfa = thompson_construction(re_string)\n",
    "    thompson_nfa.render('thompson_nfa', cleanup=True)\n",
    "    print(\"Thompson NFA saved as 'thompson_nfa.png'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glushkov_construction(re_string):\n",
    "    \"\"\"\n",
    "    Generates a Glushkov NFA for the given regular expression.\n",
    "    \"\"\"\n",
    "    dot = Digraph(format='png')\n",
    "    dot.attr(rankdir='LR')  # Left-to-right visualization\n",
    "\n",
    "    # Assign unique positions to each symbol in the regular expression\n",
    "    positions = {m.start(): m.group() for m in re.finditer(r'[a-zA-Z]', re_string)}\n",
    "    start_state = 'q0'\n",
    "    dot.node(start_state, start_state, shape='circle', color='red')  # Start state\n",
    "\n",
    "    # Add transitions based on the sequence of symbols\n",
    "    prev_state = start_state\n",
    "    for pos, char in positions.items():\n",
    "        next_state = f'q{pos + 1}'\n",
    "        dot.node(next_state, next_state, shape='circle')\n",
    "        dot.edge(prev_state, next_state, label=char)\n",
    "        prev_state = next_state\n",
    "\n",
    "    # Add a final state\n",
    "    final_state = f'q{len(positions) + 1}'\n",
    "    dot.node(final_state, final_state, shape='doublecircle', color='green')  # Final state\n",
    "    dot.edge(prev_state, final_state, label='ε')  # Transition to final state\n",
    "\n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "# Regular expressions\n",
    "regex1 = r\"(aba|ba|aaa)+\"\n",
    "regex2 = r\"(ab|abc|aba)+ba(c+)(cbb|bba|a)+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequences for Regex 1:\n",
      "['aaabaaaaabaabaaaaaaa', 'aaaaaababaabaaaaaa', 'abaaaaabababaaaaaaaababaaba', 'ababaaaabaabaaaaaaabaabaaa', 'aaaababaabaaaabaabababaaaaabaaaabaabaaaaa']\n",
      "Generated sequences for Regex 2:\n",
      "['abcbaccccbbababaccbbaabcbacccccbba', 'abbacccbbaabcbacccaabbacccaababacccccbbabab', 'abbaccccccbbabc', 'ababacccccbbabab', 'abbaccccbbaabbaccccaabbaccccaabcbaccccaababa']\n",
      "Random file 'random_sequences.txt' created with injected sequences.\n",
      "\n",
      "Matches for Regex 1: 2\n",
      "Matches for Regex 2: 0\n",
      "\n",
      "Examples of matches for Regex 1:\n",
      "['aaabaaaaabaabaaaaaaa', 'abaaaaabababaaaaaaaababaaba']\n",
      "\n",
      "Examples of matches for Regex 2:\n",
      "[]\n",
      "\n",
      "Estimated frequency for Regex 1: 0.0020\n",
      "Estimated frequency for Regex 2: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Generate sequences that belong to the given regular expressions\n",
    "def generate_sequences(regex, num_sequences=10, min_length=3, max_length=60):\n",
    "    if regex == regex1:\n",
    "        base_patterns = [\"aba\", \"ba\", \"aaa\"]\n",
    "    elif regex == regex2:\n",
    "        base_patterns = [\"ab\", \"abc\", \"aba\"]\n",
    "        tail_patterns = [\"cbb\", \"bba\", \"a\"]\n",
    "    \n",
    "    sequences = []\n",
    "    for _ in range(num_sequences):\n",
    "        length = random.randint(min_length, max_length)\n",
    "        sequence = \"\"\n",
    "        while len(sequence) < length:\n",
    "            if regex == regex1:\n",
    "                sequence += random.choice(base_patterns)\n",
    "            elif regex == regex2:\n",
    "                sequence += random.choice(base_patterns)  # First part\n",
    "                sequence += \"ba\"  # Fixed 'ba'\n",
    "                sequence += \"c\" * random.randint(1, 5)  # c+\n",
    "                sequence += random.choice(tail_patterns)  # Tail patterns\n",
    "        sequences.append(sequence[:length])\n",
    "    return sequences\n",
    "\n",
    "# Generate random sequences and inject examples\n",
    "def generate_random_file(filename, alphabet=\"abc\", num_sequences=1000, sequence_length=50, injected_sequences=[]):\n",
    "    with open(filename, \"w\") as file:\n",
    "        for _ in range(num_sequences - len(injected_sequences)):\n",
    "            sequence = \"\".join(random.choice(alphabet) for _ in range(sequence_length))\n",
    "            file.write(sequence + \"\\n\")\n",
    "        for seq in injected_sequences:\n",
    "            file.write(seq + \"\\n\")\n",
    "\n",
    "# Match regular expressions against the file\n",
    "def match_nfa_against_file(regex, filename):\n",
    "    pattern = re.compile(regex)\n",
    "    matches = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            sequence = line.strip()\n",
    "            if pattern.fullmatch(sequence):\n",
    "                matches.append(sequence)\n",
    "    return matches\n",
    "\n",
    "# Estimate frequency of matches\n",
    "def estimate_frequency(num_matches, total_sequences):\n",
    "    return num_matches / total_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfa_to_dfa(nfa_transitions, start_state, accept_states, alphabet):\n",
    "    \"\"\"\n",
    "    Converts an NFA to a DFA using subset construction.\n",
    "\n",
    "    Parameters:\n",
    "        nfa_transitions: Dict of NFA transitions {state: {symbol: [next_states]}}\n",
    "        start_state: Initial state of the NFA\n",
    "        accept_states: Set of accepting states in the NFA\n",
    "        alphabet: List of input symbols\n",
    "\n",
    "    Returns:\n",
    "        A tuple (dfa_transitions, dfa_start_state, dfa_accept_states)\n",
    "    \"\"\"\n",
    "    # Helper function: Compute epsilon-closure of a set of states\n",
    "    def epsilon_closure(states, nfa_transitions):\n",
    "        stack = list(states)\n",
    "        closure = set(states)\n",
    "\n",
    "        while stack:\n",
    "            state = stack.pop()\n",
    "            if state in nfa_transitions and 'ε' in nfa_transitions[state]:\n",
    "                for next_state in nfa_transitions[state]['ε']:\n",
    "                    if next_state not in closure:\n",
    "                        closure.add(next_state)\n",
    "                        stack.append(next_state)\n",
    "        return closure\n",
    "\n",
    "    # DFA structures\n",
    "    dfa_transitions = {}\n",
    "    dfa_start_state = frozenset(epsilon_closure([start_state], nfa_transitions))\n",
    "    dfa_states = {dfa_start_state}\n",
    "    unmarked_states = [dfa_start_state]\n",
    "    dfa_accept_states = set()\n",
    "\n",
    "    while unmarked_states:\n",
    "        current_dfa_state = unmarked_states.pop()\n",
    "        dfa_transitions[current_dfa_state] = {}\n",
    "\n",
    "        # Check if this DFA state is an accept state\n",
    "        if any(state in accept_states for state in current_dfa_state):\n",
    "            dfa_accept_states.add(current_dfa_state)\n",
    "\n",
    "        # Compute transitions for each symbol in the alphabet\n",
    "        for symbol in alphabet:\n",
    "            next_states = set()\n",
    "            for nfa_state in current_dfa_state:\n",
    "                if nfa_state in nfa_transitions and symbol in nfa_transitions[nfa_state]:\n",
    "                    next_states.update(nfa_transitions[nfa_state][symbol])\n",
    "\n",
    "            # Compute epsilon-closure of the reachable states\n",
    "            next_closure = frozenset(epsilon_closure(next_states, nfa_transitions))\n",
    "            if next_closure:\n",
    "                dfa_transitions[current_dfa_state][symbol] = next_closure\n",
    "                if next_closure not in dfa_states:\n",
    "                    dfa_states.add(next_closure)\n",
    "                    unmarked_states.append(next_closure)\n",
    "\n",
    "    return dfa_transitions, dfa_start_state, dfa_accept_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_dfa(dfa_transitions, dfa_start_state, dfa_accept_states, alphabet):\n",
    "    \"\"\"\n",
    "    Minimizes a DFA.\n",
    "\n",
    "    Parameters:\n",
    "        dfa_transitions: Dict of DFA transitions {state: {symbol: next_state}}\n",
    "        dfa_start_state: Initial state of the DFA\n",
    "        dfa_accept_states: Set of accepting states in the DFA\n",
    "        alphabet: List of input symbols\n",
    "\n",
    "    Returns:\n",
    "        A tuple (min_dfa_transitions, min_dfa_start_state, min_dfa_accept_states)\n",
    "    \"\"\"\n",
    "    # Partition states into accepting and non-accepting sets\n",
    "    all_states = set(dfa_transitions.keys())\n",
    "    partition = [dfa_accept_states, all_states - dfa_accept_states]\n",
    "\n",
    "    # Refinement of partitions\n",
    "    while True:\n",
    "        new_partition = []\n",
    "        for group in partition:\n",
    "            # Split group based on transitions\n",
    "            split = defaultdict(set)\n",
    "            for state in group:\n",
    "                transitions = tuple(\n",
    "                    next((i for i, part in enumerate(partition) if dfa_transitions.get(state, {}).get(symbol) in part), None)\n",
    "                    for symbol in alphabet\n",
    "                )\n",
    "                split[transitions].add(state)\n",
    "            new_partition.extend(split.values())\n",
    "\n",
    "        if new_partition == partition:\n",
    "            break\n",
    "        partition = new_partition\n",
    "\n",
    "    # Map old states to new states\n",
    "    state_map = {state: i for i, group in enumerate(partition) for state in group}\n",
    "\n",
    "    # Build the minimized DFA\n",
    "    min_dfa_transitions = {}\n",
    "    min_dfa_accept_states = set()\n",
    "    min_dfa_start_state = state_map[dfa_start_state]\n",
    "\n",
    "    for state, transitions in dfa_transitions.items():\n",
    "        new_state = state_map[state]\n",
    "        if new_state not in min_dfa_transitions:\n",
    "            min_dfa_transitions[new_state] = {}\n",
    "        for symbol, next_state in transitions.items():\n",
    "            min_dfa_transitions[new_state][symbol] = state_map[next_state]\n",
    "\n",
    "    for group in partition:\n",
    "        if group & dfa_accept_states:\n",
    "            min_dfa_accept_states.add(state_map[next(iter(group))])\n",
    "\n",
    "    return min_dfa_transitions, min_dfa_start_state, min_dfa_accept_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main program\n",
    "def main():\n",
    "    # Generate sequences for each regex\n",
    "    sequences1 = generate_sequences(regex1, num_sequences=5, min_length=3, max_length=50)\n",
    "    sequences2 = generate_sequences(regex2, num_sequences=5, min_length=10, max_length=60)\n",
    "    injected_sequences = sequences1 + sequences2\n",
    "    \n",
    "    print(\"Generated sequences for Regex 1:\")\n",
    "    print(sequences1)\n",
    "    print(\"Generated sequences for Regex 2:\")\n",
    "    print(sequences2)\n",
    "    \n",
    "    # Generate a random file and inject sequences\n",
    "    random_filename = \"random_sequences.txt\"\n",
    "    generate_random_file(random_filename, injected_sequences=injected_sequences)\n",
    "    print(f\"Random file '{random_filename}' created with injected sequences.\")\n",
    "    \n",
    "    # Match regex 1 and regex 2 against the file\n",
    "    matches1 = match_nfa_against_file(regex1, random_filename)\n",
    "    matches2 = match_nfa_against_file(regex2, random_filename)\n",
    "    \n",
    "    print(f\"\\nMatches for Regex 1: {len(matches1)}\")\n",
    "    print(f\"Matches for Regex 2: {len(matches2)}\")\n",
    "    \n",
    "    # Display some matches\n",
    "    print(\"\\nExamples of matches for Regex 1:\")\n",
    "    print(matches1[:5])\n",
    "    print(\"\\nExamples of matches for Regex 2:\")\n",
    "    print(matches2[:5])\n",
    "    \n",
    "    # Estimate frequency of matches\n",
    "    total_sequences = 1000\n",
    "    frequency1 = estimate_frequency(len(matches1), total_sequences)\n",
    "    frequency2 = estimate_frequency(len(matches2), total_sequences)\n",
    "    print(f\"\\nEstimated frequency for Regex 1: {frequency1:.4f}\")\n",
    "    print(f\"Estimated frequency for Regex 2: {frequency2:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
